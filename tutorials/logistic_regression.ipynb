{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In machine learning, logistic regression, or logit model, is a regression model that is typically used for binary classification tasks. For instance, to predict if an email is a spam or not, or to distinguish whether a grid of pixel intensities represents a “0” digit or a “1” digit. \n",
    "\n",
    "Formally, given a dataset $X=[x_1|x_2|\\cdots|x_n]^T$ of $n$ elements, we want to predict each corresponding label $y_i\\in\\{0,1\\}$ from $x_i$. In the two examples mentioned above, $x_i$ is either the contents of each email, or the pixel intensities of each given figure. In the first case, $y_i=1$ represents the email is spam, and $y_i=0$ otherwise. In the second case, $y_i$ is simply the digit value.\n",
    "\n",
    "\n",
    "## Model specification:\n",
    "\n",
    "In logistic regression we use a hypothesis class to try to predict the probability that a given example belongs to the “1” class versus the probability that it belongs to the “0” class. Specifically, we will try to learn the conditional probability represented in the form:\n",
    "\n",
    "$P(y=1|x)=h_{\\beta}(x)=\\frac{1}{1+\\exp(−\\beta^{\\top}x)}\\equiv \\sigma(\\beta^{\\top}x),$\n",
    "\n",
    "$P(y=0|x)=1−P(y=1|x)=1−h_{\\beta}(x).$\n",
    "\n",
    "The value $\\beta^{\\top}x$ is the linear prediction we get from the model $\\beta$ and a data point $x$. The larger this value is, the more possible $x$ belongs to category $1$. To \"squash\" this value to $[0,1]$ range so that we could interpret this value as probability, we use this function $\\sigma(z)\\equiv 1+\\exp(−z)$, also called the \"sigmoid\" or \"logistic\" function. Our goal is to search for a value of $\\beta$ so that the probability $P(y=1|x)=h_{\\beta}(x)$ is large when $x$ belongs to the \"1\" class and small when $x$ belongs to the \"0\" class (so that $\\Pr(y=0|x)$ is large). For a set of training examples with binary labels $\\{(x(i),y(i)):i=1,\\cdots,m\\}$ the following cost function measures how well a given $h_{\\beta}$ does this:\n",
    "\n",
    "$J(\\beta)=−\\sum_i(y_i\\log(h_{\\beta}(x_i))+(1−y_i)\\log(1−h_{\\beta}(x_i))).$\n",
    "\n",
    "\n",
    "\n",
    "## Data handling\n",
    "We use a dataset from Kaggle competition [Santander Customer Satisfaction](https://www.kaggle.com/c/santander-customer-satisfaction). In this example, the labels are in the last \"TARGET\" column, and each sample has 368 features with no further descriptions. We first conduct logistic regression on this task by using mxnet's NDArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020L, 368L)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import os\n",
    "import urllib\n",
    "def getData(fname):\n",
    "    data=[]\n",
    "    label=[]\n",
    "    firstline=True\n",
    "    for line in file(fname):\n",
    "        if (firstline):\n",
    "            firstline=False\n",
    "            continue\n",
    "        tks = line.strip().split(',')\n",
    "        data.append([float(i) for i in tks[1:-2]]) # omit the first column of user ID.\n",
    "        label.append(int(tks[-1]))\n",
    "    return mx.nd.array(data),mx.nd.array(label)\n",
    "if not os.path.exists('train.csv'):\n",
    "    urllib.urlretrieve('https://www.kaggle.com/c/santander-customer-satisfaction/download/train.csv.zip', 'train.csv')\n",
    "if not os.path.exists('test.csv'):\n",
    "    urllib.urlretrieve('https://www.kaggle.com/c/santander-customer-satisfaction/download/test.csv.zip', 'test.csv')\n",
    "X_train,y_train=getData('train.csv')\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocess\n",
    "For simpler calculations, we add an all one feature to each sample to avoid the use of constant: $\\beta^T x + b \\Rightarrow [\\beta, b]^T[x,1]$. MXNet NDArray has the concatenate attribute designed for this task.\n",
    "\n",
    "We reshape the labels \"y_train\" as a nx1 2D array for the sake of mxnet ndarray multiplication. (Otherwise y_train will only have one dimension and won't be able to multiply with a 2D NDArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n,p = X_train.shape\n",
    "n,p\n",
    "n=int(n)\n",
    "p=int(p)\n",
    "X_train=mx.nd.concatenate([X_train.T,mx.nd.ones((1,n))]).T # add the terms for constant\n",
    "p+=1\n",
    "n,p\n",
    "y_train=y_train.reshape((n,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Optimization\n",
    "### Step-by-step implementation by NDArray\n",
    "Logistic regression maximizes the log-likelihood function. In order to conduct gradient descent on $J(\\beta)$, we compute its gradient $g$ first:\n",
    "$g\\equiv\\nabla_{\\beta}J(\\beta)=\\sum_ix_i(h_{\\beta}(x_i)−y_i)= \\sum_ix_i\\big(\\frac{1}{1+\\exp(−\\beta^{\\top}x_i)}-y_i\\big)=X^T(1./(1+\\exp(-X\\beta))-y)$, where $./$ is coordinate-wise division.\n",
    "\n",
    "With the help of MXNet NDArray, we could compute the gradience step by step as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NDArray 369x1 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialization\n",
    "beta=mx.nd.zeros((p,1))\n",
    "# multiplication\n",
    "predy=mx.nd.dot(X_train,beta)\n",
    "# exponential\n",
    "exppredy=mx.nd.exp(-predy)\n",
    "# entry-wise division\n",
    "h=mx.nd.divide(mx.nd.ones((n,1)),exppredy+1)\n",
    "tmp=h-y_train\n",
    "g=mx.nd.dot(mx.nd.transpose(X_train),tmp)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to avoid overfitting, we add an $\\ell_2$ regularization $\\alpha\\|\\beta\\|_2^2$ to the loss function $J(\\beta)$, therefore in the gradient descent step, we do:\n",
    "$\\beta\\leftarrow \\beta-2\\alpha\\beta-g=(1-2\\alpha)\\beta-\\eta g$, where $\\eta$ is the learning rate.\n",
    "\n",
    "Therefore the whole training process of logistic regression becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: training auc: 0.6496\n",
      "iter 1: training auc: 0.6499\n",
      "iter 2: training auc: 0.6502\n",
      "iter 3: training auc: 0.6506\n",
      "iter 4: training auc: 0.6510\n",
      "iter 5: training auc: 0.6515\n",
      "iter 6: training auc: 0.6523\n",
      "iter 7: training auc: 0.6539\n",
      "iter 8: training auc: 0.6562\n",
      "iter 9: training auc: 0.6575\n",
      "iter 10: training auc: 0.6581\n",
      "iter 11: training auc: 0.6601\n",
      "iter 12: training auc: 0.6616\n",
      "iter 13: training auc: 0.6624\n",
      "iter 14: training auc: 0.6630\n",
      "iter 15: training auc: 0.6653\n",
      "iter 16: training auc: 0.7220\n",
      "iter 17: training auc: 0.7267\n",
      "iter 18: training auc: 0.7275\n",
      "iter 19: training auc: 0.7287\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def train(X_train, y_train):\n",
    "    beta=mx.nd.zeros((p,1))\n",
    "    itr = 20\n",
    "    alpha=0.01\n",
    "    learn_rate = 1./n\n",
    "    predy=mx.nd.dot(X_train,beta)\n",
    "    for i in range(itr):\n",
    "        exppredy=mx.nd.exp(-predy)\n",
    "        tmp=mx.nd.divide(mx.nd.ones((n,1)),exppredy+1)-y_train\n",
    "        g=mx.nd.dot(mx.nd.transpose(X_train),tmp)\n",
    "        beta=beta*(1-2*alpha)-learn_rate*g\n",
    "        predy=mx.nd.dot(X_train,beta)\n",
    "        auc=roc_auc_score(y_train.asnumpy(), predy.asnumpy())\n",
    "        print \"iter %d: training auc: %.4f\"%(i, auc)\n",
    "    return beta\n",
    "beta=train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Symbol manipulation\n",
    "Another important object in MXNet is Symbol provided by mxnet.symbol, or mxnet.sym for short. \n",
    "A symbol represents a multi-output symbolic expression. They are composited by operators, such as simple matrix operations (e.g. “+”), or a neural network layer (e.g. convolution layer). Details can be found in http://mxnet.io/tutorials/python/symbol.html.\n",
    "\n",
    "Therefore, after using the NDArray to design the algorithm for logistic regression, we could also implement the whole process using mxnet.sym. \n",
    "\n",
    "We copy the function \"train\", but only change all the \"mx.nd\" to \"mx.sym\". Notice mxnet.symbol inherits the operations supported by mx.ndarray, and functions in a symbolic way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train):\n",
    "    beta=mx.sym.zeros((p,1))\n",
    "    itr = 20\n",
    "    alpha=0.01\n",
    "    learn_rate = 1./n\n",
    "    predy=mx.sym.dot(X_train,beta)\n",
    "    for i in range(itr):\n",
    "        exppredy=mx.sym.exp(-predy)\n",
    "        tmp=mx.sym.ones((n,1))/(exppredy+1)-y_train\n",
    "        g=mx.sym.dot(mx.sym.transpose(X_train),tmp)\n",
    "        beta=beta*(1-2*alpha)-learn_rate*g\n",
    "        predy=mx.sym.dot(X_train,beta)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Bind with data and evaluate:\n",
    "After defining the train function, we can run it and get a symbolic output.\n",
    "Notice this whole process only declares computation, we need to bind with data to run.\n",
    "In the following, we bind with the same data as before and get the same result we got using NDArray: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training auc: 0.7287\n"
     ]
    }
   ],
   "source": [
    "X_train_sym = mx.sym.Variable('X_train')\n",
    "y_train_sym = mx.sym.Variable('y_train')\n",
    "beta_sym=train(X_train_sym,y_train_sym)\n",
    "ex=beta_sym.bind(ctx=mx.cpu(), args={'X_train':X_train, 'y_train':y_train})\n",
    "ex.forward()\n",
    "beta=ex.outputs[0]\n",
    "auc=roc_auc_score(y_train.asnumpy(), mx.nd.dot(X_train,beta).asnumpy())\n",
    "print \"Final training auc: %.4f\"%(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Module auto differentiation\n",
    "Instead of computing the gradient manually, mxnet.symbol actually supports auto differentiation. \n",
    "We first define a single layered network with a fully connected layer and logistic regression output, and show how mxnet.mod.Module supports train, predict and evaluate automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: plot Pages: 1 -->\n",
       "<svg width=\"214pt\" height=\"282pt\"\n",
       " viewBox=\"0.00 0.00 214.00 282.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 278)\">\n",
       "<title>plot</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-278 210,-278 210,4 -4,4\"/>\n",
       "<!-- data -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>data</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"#000000\" cx=\"47\" cy=\"-29\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-24.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">data</text>\n",
       "</g>\n",
       "<!-- fc1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>fc1</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"#000000\" points=\"94,-166 0,-166 0,-108 94,-108 94,-166\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-139.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-125.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1</text>\n",
       "</g>\n",
       "<!-- fc1&#45;&gt;data -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>fc1&#45;&gt;data</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47,-97.6567C47,-84.6329 47,-70.3785 47,-58.2497\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47,-107.7736 42.5001,-97.7736 47,-102.7736 47.0001,-97.7736 47.0001,-97.7736 47.0001,-97.7736 47,-102.7736 51.5001,-97.7737 47,-107.7736 47,-107.7736\"/>\n",
       "<text text-anchor=\"middle\" x=\"57.5\" y=\"-78.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">369</text>\n",
       "</g>\n",
       "<!-- softmax_label -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>softmax_label</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"#000000\" cx=\"159\" cy=\"-137\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-132.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">softmax_label</text>\n",
       "</g>\n",
       "<!-- softmax -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>softmax</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"#000000\" points=\"172,-274 78,-274 78,-216 172,-216 172,-274\"/>\n",
       "<text text-anchor=\"middle\" x=\"125\" y=\"-240.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">LogisticRegressionOutput</text>\n",
       "</g>\n",
       "<!-- softmax&#45;&gt;fc1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>softmax&#45;&gt;fc1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M97.9755,-207.5815C88.2024,-194.0495 77.3181,-178.9789 68.1248,-166.2497\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.8921,-215.7736 94.3891,-210.3016 100.9646,-211.7202 98.0371,-207.6668 98.0371,-207.6668 98.0371,-207.6668 100.9646,-211.7202 101.6852,-205.0321 103.8921,-215.7736 103.8921,-215.7736\"/>\n",
       "<text text-anchor=\"middle\" x=\"94.5\" y=\"-186.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1</text>\n",
       "</g>\n",
       "<!-- softmax&#45;&gt;softmax_label -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>softmax&#45;&gt;softmax_label</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M137.3156,-205.8799C141.519,-192.5278 146.1369,-177.8593 150.0269,-165.5026\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"134.2009,-215.7736 132.9115,-204.8838 135.7024,-211.0044 137.2038,-206.2351 137.2038,-206.2351 137.2038,-206.2351 135.7024,-211.0044 141.4961,-207.5865 134.2009,-215.7736 134.2009,-215.7736\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x111a55890>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "net = mx.sym.Variable('data')\n",
    "net = mx.sym.FullyConnected(data=net, name='fc1', num_hidden=1)\n",
    "net = mx.sym.LogisticRegressionOutput(data=net, name='softmax')\n",
    "shape = {\"data\" : (1, p)}\n",
    "mx.viz.plot_network(symbol=net, shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataDesc[data,(1, 369L),<type 'numpy.float32'>,NCHW]]\n",
      "('accuracy', 0.9474217311233886)\n"
     ]
    }
   ],
   "source": [
    "mod = mx.mod.Module(symbol=net)\n",
    "train_iter=mx.io.NDArrayIter(data=X_train,label=y_train,batch_size=1)\n",
    "print(train_iter.provide_data)\n",
    "mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)\n",
    "# init parameters\n",
    "mod.init_params(initializer=mx.init.Xavier(magnitude=0.01))\n",
    "\n",
    "# init optimizer\n",
    "mod.init_optimizer(optimizer='sgd', optimizer_params=(('learning_rate', 1./n), ))\n",
    "\n",
    "# use accuracy as the metric\n",
    "metric = mx.metric.create('acc')\n",
    "\n",
    "# train one epoch, i.e. going over the data iter one pass\n",
    "for batch in train_iter:\n",
    "    mod.forward(batch, is_train=True)       # compute predictions\n",
    "    mod.update_metric(metric, batch.label)  # accumulate prediction accuracy\n",
    "    mod.backward()                          # compute gradients\n",
    "    mod.update()                            # update parameters using SGD\n",
    "    \n",
    "# training error\n",
    "print(metric.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, similar to the linear regression, logistic regression is easily trained by calling the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy', 0.9539265982636148)\n"
     ]
    }
   ],
   "source": [
    "mod = mx.mod.Module(symbol=net)\n",
    "mod.fit(train_iter, eval_data=train_iter, optimizer='sgd',optimizer_params={'learning_rate':1./n},eval_metric='mse',num_epoch=20)\n",
    "for batch in train_iter:\n",
    "    mod.update_metric(metric,batch.label)\n",
    "print(metric.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the result from what we got by manually training the logistic regression model, we again output the accuracy measured by roc auc score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70665938057\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_train.asnumpy(),mod.predict(train_iter).asnumpy()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
