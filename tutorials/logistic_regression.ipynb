{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In machine learning, logistic regression, or logit model, is a regression model that is typically used for binary classification tasks. For instance, to predict if an email is a spam or not, or to distinguish whether a grid of pixel intensities represents a “0” digit or a “1” digit. \n",
    "\n",
    "Formally, given a dataset $X=[x_1|x_2|\\cdots|x_n]^T$ of $n$ elements, we want to predict each corresponding label $y_i\\in\\{0,1\\}$ from $x_i$. In the two examples mentioned above, $x_i$ is either the contents of each email, or the pixel intensities of each given figure. In the first case, $y_i=1$ represents the email is spam, and $y_i=0$ otherwise. In the second case, $y_i$ is simply the digit value.\n",
    "\n",
    "\n",
    "## Model specification:\n",
    "\n",
    "In logistic regression we use a hypothesis class to try to predict the probability that a given example belongs to the “1” class versus the probability that it belongs to the “0” class. Specifically, we will try to learn the conditional probability represented in the form:\n",
    "\n",
    "$P(y=1|x)=h_{\\beta}(x)=\\frac{1}{1+\\exp(−\\beta^{\\top}x)}\\equiv \\sigma(\\beta^{\\top}x),$\n",
    "\n",
    "$P(y=0|x)=1−P(y=1|x)=1−h_{\\beta}(x).$\n",
    "\n",
    "The value $\\beta^{\\top}x$ is the linear prediction we get from the model $\\beta$ and a data point $x$. The larger this value is, the more possible $x$ belongs to category $1$. To \"squash\" this value to $[0,1]$ range so that we could interpret this value as probability, we use this function $\\sigma(z)\\equiv 1+\\exp(−z)$, also called the \"sigmoid\" or \"logistic\" function. Our goal is to search for a value of $\\beta$ so that the probability $P(y=1|x)=h_{\\beta}(x)$ is large when $x$ belongs to the \"1\" class and small when $x$ belongs to the \"0\" class (so that $\\Pr(y=0|x)$ is large). For a set of training examples with binary labels $\\{(x(i),y(i)):i=1,\\cdots,m\\}$ the following cost function measures how well a given $h_{\\beta}$ does this:\n",
    "\n",
    "$J(\\beta)=−\\sum_i(y_i\\log(h_{\\beta}(x_i))+(1−y_i)\\log(1−h_{\\beta}(x_i))).$\n",
    "\n",
    "\n",
    "\n",
    "## Data handling\n",
    "We use a dataset from Kaggle competition [Santander Customer Satisfaction](https://www.kaggle.com/c/santander-customer-satisfaction). In this example, the labels are in the last \"TARGET\" column, and each sample has 368 features with no further descriptions. We first conduct logistic regression on this task by using mxnet's NDArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020L, 368L)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import os\n",
    "import urllib\n",
    "def getData(fname):\n",
    "    data=[]\n",
    "    label=[]\n",
    "    firstline=True\n",
    "    for line in file(fname):\n",
    "        if (firstline):\n",
    "            firstline=False\n",
    "            continue\n",
    "        tks = line.strip().split(',')\n",
    "        data.append([float(i) for i in tks[1:-2]]) # omit the first column of user ID.\n",
    "        label.append(int(tks[-1]))\n",
    "    return mx.nd.array(data),mx.nd.array(label)\n",
    "if not os.path.exists('train.csv'):\n",
    "    urllib.urlretrieve('https://www.kaggle.com/c/santander-customer-satisfaction/download/train.csv.zip', 'train.csv')\n",
    "if not os.path.exists('test.csv'):\n",
    "    urllib.urlretrieve('https://www.kaggle.com/c/santander-customer-satisfaction/download/test.csv.zip', 'test.csv')\n",
    "X_train,y_train=getData('train.csv')\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocess\n",
    "For simpler calculations, we add an all one feature to each sample to avoid the use of constant: $\\beta^T x + b \\Rightarrow [\\beta, b]^T[x,1]$. MXNet NDArray has the concatenate attribute designed for this task.\n",
    "\n",
    "We reshape the labels \"y_train\" as a nx1 2D array for the sake of mxnet ndarray multiplication. (Otherwise y_train will only have one dimension and won't be able to multiply with a 2D NDArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n,p = X_train.shape\n",
    "n,p\n",
    "n=int(n)\n",
    "p=int(p)\n",
    "X_train=mx.nd.concatenate([X_train.T,mx.nd.ones((1,n))]).T # add the terms for constant\n",
    "p+=1\n",
    "n,p\n",
    "y_train=y_train.reshape((n,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Optimization\n",
    "### Step-by-step implementation by NDArray\n",
    "Logistic regression maximizes the log-likelihood function. In order to conduct gradient descent on $J(\\beta)$, we compute its gradient $g$ first:\n",
    "$g\\equiv\\nabla_{\\beta}J(\\beta)=\\sum_ix_i(h_{\\beta}(x_i)−y_i)= \\sum_ix_i\\big(\\frac{1}{1+\\exp(−\\beta^{\\top}x_i)}-y_i\\big)=X^T(1./(1+\\exp(-X\\beta))-y)$, where $./$ is coordinate-wise division.\n",
    "\n",
    "With the help of MXNet NDArray, we could compute the gradience step by step as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NDArray 369x1 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialization\n",
    "beta=mx.nd.zeros((p,1))\n",
    "# multiplication\n",
    "predy=mx.nd.dot(X_train,beta)\n",
    "# exponential\n",
    "exppredy=mx.nd.exp(-predy)\n",
    "# entry-wise division\n",
    "h=mx.nd.divide(mx.nd.ones((n,1)),exppredy+1)\n",
    "tmp=h-y_train\n",
    "g=mx.nd.dot(mx.nd.transpose(X_train),tmp)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to avoid overfitting, we add an $\\ell_2$ regularization $\\alpha\\|\\beta\\|_2^2$ to the loss function $J(\\beta)$, therefore in the gradient descent step, we do:\n",
    "$\\beta\\leftarrow \\beta-2\\alpha\\beta-g=(1-2\\alpha)\\beta-\\eta g$, where $\\eta$ is the learning rate.\n",
    "\n",
    "Therefore the whole training process of logistic regression becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: training auc: 0.6496\n",
      "iter 1: training auc: 0.6499\n",
      "iter 2: training auc: 0.6502\n",
      "iter 3: training auc: 0.6506\n",
      "iter 4: training auc: 0.6510\n",
      "iter 5: training auc: 0.6515\n",
      "iter 6: training auc: 0.6523\n",
      "iter 7: training auc: 0.6539\n",
      "iter 8: training auc: 0.6562\n",
      "iter 9: training auc: 0.6575\n",
      "iter 10: training auc: 0.6581\n",
      "iter 11: training auc: 0.6601\n",
      "iter 12: training auc: 0.6616\n",
      "iter 13: training auc: 0.6624\n",
      "iter 14: training auc: 0.6630\n",
      "iter 15: training auc: 0.6653\n",
      "iter 16: training auc: 0.7220\n",
      "iter 17: training auc: 0.7267\n",
      "iter 18: training auc: 0.7275\n",
      "iter 19: training auc: 0.7287\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def train(X_train, y_train):\n",
    "    beta=mx.nd.zeros((p,1))\n",
    "    itr = 20\n",
    "    alpha=0.01\n",
    "    learn_rate = 1./n\n",
    "    predy=mx.nd.dot(X_train,beta)\n",
    "    for i in range(itr):\n",
    "        exppredy=mx.nd.exp(-predy)\n",
    "        tmp=mx.nd.divide(mx.nd.ones((n,1)),exppredy+1)-y_train\n",
    "        g=mx.nd.dot(mx.nd.transpose(X_train),tmp)\n",
    "        beta=beta*(1-2*alpha)-learn_rate*g\n",
    "        predy=mx.nd.dot(X_train,beta)\n",
    "        auc=roc_auc_score(y_train.asnumpy(), predy.asnumpy())\n",
    "        print \"iter %d: training auc: %.4f\"%(i, auc)\n",
    "    return beta\n",
    "beta=train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Symbol manipulation\n",
    "Another important object in MXNet is Symbol provided by mxnet.symbol, or mxnet.sym for short. \n",
    "A symbol represents a multi-output symbolic expression. They are composited by operators, such as simple matrix operations (e.g. “+”), or a neural network layer (e.g. convolution layer). Details can be found in http://mxnet.io/tutorials/python/symbol.html.\n",
    "\n",
    "Therefore, after using the NDArray to design the algorithm for logistic regression, we could also implement the whole process using mxnet.sym. \n",
    "\n",
    "We copy the function \"train\", but only change all the \"mx.nd\" to \"mx.sym\". Notice mxnet.symbol inherits the operations supported by mx.ndarray, and functions in a symbolic way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train):\n",
    "    beta=mx.sym.zeros((p,1))\n",
    "    itr = 20\n",
    "    alpha=0.01\n",
    "    learn_rate = 1./n\n",
    "    predy=mx.sym.dot(X_train,beta)\n",
    "    for i in range(itr):\n",
    "        exppredy=mx.sym.exp(-predy)\n",
    "        tmp=mx.sym.ones((n,1))/(exppredy+1)-y_train\n",
    "        g=mx.sym.dot(mx.sym.transpose(X_train),tmp)\n",
    "        beta=beta*(1-2*alpha)-learn_rate*g\n",
    "        predy=mx.sym.dot(X_train,beta)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Bind with data and evaluate:\n",
    "After defining the train function, we can run it and get a symbolic output.\n",
    "Notice this whole process only declares computation, we need to bind with data to run.\n",
    "In the following, we bind with the same data as before and get the same result we got using NDArray: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training auc: 0.7287\n"
     ]
    }
   ],
   "source": [
    "X_train_sym = mx.sym.Variable('X_train')\n",
    "y_train_sym = mx.sym.Variable('y_train')\n",
    "beta_sym=train(X_train_sym,y_train_sym)\n",
    "ex=beta_sym.bind(ctx=mx.cpu(), args={'X_train':X_train, 'y_train':y_train})\n",
    "ex.forward()\n",
    "beta=ex.outputs[0]\n",
    "auc=roc_auc_score(y_train.asnumpy(), mx.nd.dot(X_train,beta).asnumpy())\n",
    "print \"Final training auc: %.4f\"%(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module auto differentiation\n",
    "Instead of computing the gradient explicitly, mxnet.symbol actually supports auto differentiation. \n",
    "We first define a single layered network with softmax output, which is equivalent to logistic regression, and show how mxnet.mod.Module supports train, predict and evaluate automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'fc1_weight', 'fc1_bias', 'softmax_label']\n",
      "['softmax_output']\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "net = mx.sym.Variable('data')\n",
    "net = mx.sym.FullyConnected(data=net, name='fc1', num_hidden=1)\n",
    "net = mx.sym.SoftmaxOutput(data=net, name='softmax')\n",
    "print(net.list_arguments())\n",
    "print(net.list_outputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataDesc[data,(1, 369L),<type 'numpy.float32'>,NCHW]]\n",
      "('accuracy', 0.0395685345961589)\n"
     ]
    }
   ],
   "source": [
    "mod = mx.mod.Module(symbol=net)\n",
    "train_iter=mx.io.NDArrayIter(data=X_train,label=y_train,batch_size=1)\n",
    "print(train_iter.provide_data)\n",
    "mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)\n",
    "# init parameters\n",
    "mod.init_params(initializer=mx.init.Xavier(magnitude=0.01))\n",
    "\n",
    "# init optimizer\n",
    "mod.init_optimizer(optimizer='sgd', optimizer_params=(('learning_rate', 1./n), ))\n",
    "\n",
    "# use accuracy as the metric\n",
    "metric = mx.metric.create('acc')\n",
    "\n",
    "# train one epoch, i.e. going over the data iter one pass\n",
    "for batch in train_iter:\n",
    "    mod.forward(batch, is_train=True)       # compute predictions\n",
    "    mod.update_metric(metric, batch.label)  # accumulate prediction accuracy\n",
    "    mod.backward()                          # compute gradients\n",
    "    mod.update()                            # update parameters using SGD\n",
    "    \n",
    "# training error\n",
    "print(metric.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Similar to the linear regression, logistic regression is easily trained by calling the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy', 0.0395685345961589)\n"
     ]
    }
   ],
   "source": [
    "mod = mx.mod.Module(symbol=net)\n",
    "mod.fit(train_iter, eval_data=train_iter, optimizer='sgd',optimizer_params={'learning_rate':1./n},eval_metric='mse',num_epoch=20)\n",
    "for batch in train_iter:\n",
    "    mod.update_metric(metric,batch.label)\n",
    "print(metric.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_train.asnumpy(),mod.predict(train_iter).asnumpy()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
